# Evaluation Methods

## Research Questions & Hypotheses
- **Research Question**: Describes or explains a relationship between variables; must be specific, focused, and yield complex answers.  
- **Variables**:  
  - **Independent Variable (IV)**: Manipulated factor (e.g., input device, context).  
  - **Dependent Variable (DV)**: Measured outcome (e.g., task time, error rate, satisfaction).  
  - **Controlled Variables**: Kept constant across trials (e.g., lighting).  
  - **Random Variables**: Allowed to vary unpredictably (e.g., time of day).  
  - **Confounding Variables**: Unaccounted factors that correlate with IV and DV; mitigate via randomization or control.  
- **Hypothesis**: Concrete, testable prediction derived from the research question; used to reject the null hypothesis (no effect).

## Study Design
- **Experimental Types**:  
  - **Within-Subject**: Each participant experiences all conditions; fewer participants needed but risk of learning effects.  
  - **Between-Subject**: Each participant experiences one condition; avoids carry-over effects but requires more participants.  
- **Counterbalancing**: Orders of conditions are varied (e.g., Balanced Latin Square) to distribute learning and carry-over effects evenly.
- **Participant Sampling**:  
  - Choose representative participants.  
  - Estimate sample size to avoid underpowered or overpowered studies.

## Evaluating with Users
- **Lab Studies**: Controlled environment to isolate cause–effect relationships.  
- **Field Studies**: Natural settings using real tasks for ecological validity.  
- **Measures**: Combine objective logging (e.g., completion time, errors) with subjective questionnaires (e.g., SUS, NASA-TLX).

## Data & Logging
- **Data Types**:  
  - **Quantitative**: Numeric (categorical: nominal, ordinal; numerical: interval, ratio).  
  - **Qualitative**: Non-numeric (interviews, observations).  
- **Logging**: Decide granularity; save raw data alongside summary metrics.  

## Questionnaires
- **Likert Scale**: Agreement ratings (typically 5–7 points, with or without neutral midpoint).  
- **Standardized**: Validated instruments for common factors.  
- **Custom**: Tailored to study needs; avoid leading, double-negative, or ambiguous questions.

## Pilots, Ethics & Effects
- **Pilot Study**: Test and refine procedures before full study.  
- **Consent & Privacy**: Inform participants, guarantee comfort and confidentiality.  
- **Hawthorne Effect**: Participants may alter behavior when observed; consider in analysis.

## Data Analysis
### Descriptive Statistics
- **Mean, Median, Mode**; **Min/Max**, **Range**, **Standard Deviation**, **Interquartile Range**, **Median Absolute Deviation**.

### Significance Testing
- **p-value**: Probability that observed effect arises by chance under the null hypothesis; p < 0.05 typically indicates significance.
- **Statistical Tests** choose based on design and data:  
  - **Parametric** (t-tests, ANOVA) when assumptions met (normality, homogeneity, scale).  
  - **Non-Parametric** (Mann-Whitney, Kruskal-Wallis, Wilcoxon, Friedman) when not.

### t-Tests
- Compare means of two groups (independent or paired).  
- Check assumptions (continuous scale, normal distribution, equal variances).

### ANOVA
- Compare means across three or more groups or conditions (one-way, repeated measures).  
- Follow up with post-hoc pairwise tests and corrections (e.g., Bonferroni).

## Limitations & Best Practices
- Experiments require clear hypotheses and controlled variables.  
- Lab settings may reduce ecological validity; field studies may introduce noise.  
- Bias and errors can never be fully eliminated—plan carefully and report transparently.

## Summary
Evaluation in HCI combines rigorous study design, careful measurement, and appropriate statistical analysis to test hypotheses about user interaction. Both quantitative and qualitative methods play key roles, and ethical, practical, and validity considerations guide every phase from pilot through publication.  
